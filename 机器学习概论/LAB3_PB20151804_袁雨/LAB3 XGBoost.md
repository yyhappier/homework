# LAB3 XGBoost

*è¢é›¨ PB20151804*



## ä¸€ã€å®éªŒç›®çš„

â€‹		å®ç°åŸºæ¨¡å‹æ˜¯å†³ç­–æ ‘çš„XGBoostã€‚å¹¶åœ¨æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒä¸æµ‹è¯•ã€‚



## äºŒã€å®éªŒåŸç†

### XGBoost

â€‹		XGBoost æ˜¯ç”±å¤šä¸ªåŸºæ¨¡å‹ç»„æˆçš„ä¸€ä¸ªåŠ æ³•æ¨¡å‹ï¼Œå‡è®¾ç¬¬ $k$ ä¸ªåŸºæœ¬æ¨¡å‹æ˜¯ $f_k (x)$, é‚£ä¹ˆå‰ $t$ ä¸ªæ¨¡å‹ç»„æˆçš„æ¨¡å‹çš„è¾“å‡ºä¸º
$$
\hat y_i^{(t)}=\sum^t_{k=1}f_k (x_i )=\hat y_i^{(t-1)}+f_t (x_i)
$$
â€‹		å…¶ä¸­ $x_i$ ä¸ºç¬¬è¡¨ç¤ºç¬¬ $i$ ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œ$y_i$ è¡¨ç¤ºç¬¬ $i$ ä¸ªæ ·æœ¬çš„çœŸå®æ ‡ç­¾;  $\hat y_i^{(t)}$ è¡¨ç¤ºå‰ $t$ ä¸ªæ¨¡å‹å¯¹ç¬¬ $i$ ä¸ªæ ·æœ¬çš„æ ‡ç­¾æœ€ç»ˆé¢„æµ‹å€¼ã€‚

â€‹		åœ¨å­¦ä¹ ç¬¬ $t$ ä¸ªåŸºæ¨¡å‹æ—¶ï¼ŒXGBoost è¦ä¼˜åŒ–çš„ç›®æ ‡å‡½æ•°ä¸º:
$$
\begin{split}
Obj^{(t)} &= \sum_{i=1}^n loss(y_i,\hat y_i^{(t)})+\sum_{k=1}^t penalty(f_k)\\
&=\sum_{i=1}^n loss(y_i,\hat y_i^{(t-1)}+f_t(x_i))+\sum_{k=1}^t penalty(f_k)\\
&=\sum_{i=1}^n loss(y_i,\hat y_i^{(t-1)}+f_t(x_i))+ penalty(f_t)+constant\\
\end{split}
$$
â€‹		å…¶ä¸­ $n$ è¡¨ç¤ºè®­ç»ƒæ ·æœ¬çš„æ•°é‡, $penalty(f_k)$ è¡¨ç¤ºå¯¹ç¬¬ $k$ ä¸ªæ¨¡å‹çš„å¤æ‚åº¦çš„æƒ©ç½šé¡¹,  $loss(y_i,\hat y_i^{(t)})$ è¡¨ç¤ºæŸå¤±å‡½æ•°ï¼Œ

â€‹		ä¾‹å¦‚äºŒåˆ†ç±»é—®é¢˜çš„ 
$$
loss(y_i, \hat y_i^{(t)} )=âˆ’y_i\cdot \logâ¡ p(\hat y_i^{(t)}=1|x_i)âˆ’(1âˆ’y_i)\logâ¡ (1-p(\hat y_i^{(t)}=1|x_i))
$$
â€‹		å›å½’é—®é¢˜
$$
loss(y_i, \hat y_i^{(t)} ))=(y_i- \hat y_i^{(t)} )^2
$$
â€‹		å°† $loss(y_i,y Ì‚_i^{(t-1) }+f_t (x_i))$ åœ¨ $y Ì‚_i^{(t-1)}$ å¤„æ³°å‹’å±•å¼€å¯å¾—
$$
loss(y_i,y Ì‚_i^{(t-1) }+f_t (x_i))â‰ˆloss(y_i,y Ì‚_i^{(t-1)} )+g_i f_t (x_i )+\frac12 h_i f_t^2 (x_i)
$$
â€‹		å…¶ä¸­ $g_i=\frac{\partial\ loss(y_i,y Ì‚_i^{(t-1)})}{\partial\  y Ì‚_i^{(t-1) } }$, $h_i=\frac{\partial^2 loss(y_i,y Ì‚_i^{(t-1)} )}{\partial \ (y Ì‚_i^{(t-1)} )^2 }\\ $ï¼Œå³ $g_i$ ä¸ºä¸€é˜¶å¯¼æ•°ï¼Œ$h_i$ ä¸ºäºŒé˜¶å¯¼æ•°ã€‚

â€‹		æ­¤æ—¶çš„ä¼˜åŒ–ç›®æ ‡å˜ä¸º
$$
Obj^{(t)}=âˆ‘_{i=1}^n[loss(y_i,y Ì‚_i^{(t-1)} )+g_i f_t (x_i )+\frac12 h_i f_t^2 (x_i)]+penalty(f_t ) +constant
$$
â€‹		å»æ‰å¸¸æ•°é¡¹ $loss(y_i,y Ì‚_i^{(t-1) })$ (å­¦ä¹ ç¬¬ $t$ ä¸ªæ¨¡å‹æ—¶å€™ï¼Œ $loss(y_i,y Ì‚_i^{(t-1) })$ ä¹Ÿæ˜¯ä¸€ä¸ªå›ºå®šå€¼) å’Œ constantï¼Œå¯å¾—ç›®æ ‡å‡½æ•°ä¸º
$$
Obj^{(t)}=\sum_{i=1}^n[g_i f_t (x_i )+\frac12 h_i f_t^2 (x_i)]+penalty(f_t )
$$

### å†³ç­–æ ‘ï¼ˆå›å½’æ ‘ï¼‰

â€‹		æœ¬å®éªŒä¸­ï¼Œæˆ‘ä»¬ä»¥å†³ç­–æ ‘ï¼ˆå›å½’æ ‘ï¼‰ä¸ºåŸºï¼Œå› æ­¤è¿˜éœ€è¦å†™å‡ºå†³ç­–æ ‘çš„ç®—æ³•ã€‚

â€‹		å‡è®¾å†³ç­–æ ‘æœ‰ $T$ ä¸ªå¶å­èŠ‚ç‚¹ï¼Œæ¯ä¸ªå¶å­èŠ‚ç‚¹å¯¹åº”æœ‰ä¸€ä¸ªæƒé‡ã€‚å†³ç­–æ ‘æ¨¡å‹å°±æ˜¯å°†è¾“å…¥ $x_i$ æ˜ å°„åˆ°æŸä¸ªå¶å­èŠ‚ç‚¹ï¼Œå†³ç­–æ ‘æ¨¡å‹çš„è¾“å‡ºå°±æ˜¯è¿™ä¸ªå¶å­èŠ‚ç‚¹çš„æƒé‡ï¼Œå³ $f(x_i )=w_{q(x_i )}$ ï¼Œ$w$ æ˜¯ä¸€ä¸ªè¦å­¦çš„ $T$ ç»´çš„å‘é‡å…¶ä¸­ $q(x_i)$ è¡¨ç¤ºæŠŠè¾“å…¥ $x_i$ æ˜ å°„åˆ°çš„å¶å­èŠ‚ç‚¹çš„ç´¢å¼•ã€‚ä¾‹å¦‚ï¼š$q(x_i )=3$ï¼Œé‚£ä¹ˆæ¨¡å‹è¾“å‡ºç¬¬ä¸‰ä¸ªå¶å­èŠ‚ç‚¹çš„æƒé‡ï¼Œå³ $f(x_i )=w_3$ã€‚

â€‹		æˆ‘ä»¬å¯¹äºæŸä¸€æ£µå†³ç­–æ ‘ï¼Œä»–çš„æƒ©ç½šä¸º
$$
penalty(f)=\gamma\cdot T+\frac12\lambda\cdot\|w\|^2
$$
â€‹		å…¶ä¸­ $\gamma,\lambda$ ä¸ºæˆ‘ä»¬å¯è°ƒæ•´çš„è¶…å‚æ•°ï¼Œ$T$ ä¸ºå¶å­æ•°ï¼Œ$w$ ä¸ºæƒé‡å‘é‡. ç”±äºæ˜¾ç¤ºé—®é¢˜ï¼Œ$\|w\|$ å®é™…ä¸Šä¸º $w$ çš„èŒƒæ•°ï¼Œä¸” $\|w\|^2=\sum_{i=1}^{dim}w_i^2$

â€‹		æˆ‘ä»¬å°†åˆ†é…åˆ°ç¬¬ $j$ ä¸ªå¶å­èŠ‚ç‚¹çš„æ ·æœ¬ç”¨ $I_j$ è¡¨ç¤ºï¼Œå³ $I_j=\{i|q(x_i )=j\} (1â‰¤jâ‰¤T)$ã€‚

â€‹		ç»¼ä¸Šï¼Œæˆ‘ä»¬åœ¨æ ‘ç»“æ„ç¡®å®šï¼ˆä½ å¯ä»¥è‡ªè¡Œç¡®å®šï¼‰æ—¶ï¼Œå¯ä»¥è¿›è¡Œå¦‚ä¸‹ä¼˜åŒ–ï¼š

$$
\begin{split}
ğ‘‚ğ‘ğ‘—^{(ğ‘¡)}&=\sum_{i=1}^n[g_i f_t (x_i )+\frac12 h_i f_t^2 (x_i)]+penalty(f_t )\\
&= \sum_{i=1}^n[g_iw_{q(x_i )} +\frac12  h_i w_{q(x_i )}^2]+\gamma \cdot T+\frac12 \lambda \cdot\|w\|^2\\
&= \sum_{j=1}^T[(\sum_{i\in I_j}g_i )â‹…ğ‘¤_ğ‘—+\frac12\cdot(\sum_{i\in I_j}h_i+\lambda )\cdot w_j^2 ]+\gamma\cdot ğ‘‡
\end{split}
$$
â€‹		ç®€å•èµ·è§ï¼Œæˆ‘ä»¬ç®€è®° $G_j=\sum_{i\in I_j}g_i , H_j=\sum_{i\in I_j}h_i $
$$
Obj^{(t)}=\sum_{j=1}^T[G_jw_j+\frac12(H_j+\lambda)w_j^2]+\gamma T
$$

 		å¯¹ $w_j$ æ±‚å¯¼ï¼Œå¾—æœ€ä¼˜çš„$w_j^*=-\frac{G_j}{H_j+\lambda} $ï¼Œå¸¦å…¥å¾—$O b j^{(t)}=-\frac{1}{2}\left(\sum_{j=1}^T \frac{G_j^2}{H_j+\lambda}\right)+\gamma T $ï¼Œä¹Ÿå°±æ˜¯æ¯ä¸ªå¶èŠ‚ç‚¹å¾—å¾—åˆ† $-\frac{1}{2} \cdot \frac{G_j^2}{H_j+\lambda}+\gamma$ ä¹‹å’Œã€‚

### æ„é€ è¿‡ç¨‹

â€‹		å¯¹äºæ¯ä¸€æ£µå†³ç­–æ ‘ï¼Œå³æ¯ä¸€ä¸ªåŸºçš„è®­ç»ƒï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤åˆ’åˆ†ç»“ç‚¹

1. ä»æ ¹èŠ‚ç‚¹å¼€å§‹é€’å½’åˆ’åˆ†ï¼Œåˆå§‹æƒ…å†µä¸‹ï¼Œæ‰€æœ‰çš„è®­ç»ƒæ ·æœ¬ $x_i$ éƒ½åˆ†é…ç»™æ ¹èŠ‚ç‚¹ã€‚

2. æ ¹æ®åˆ’åˆ†å‰åçš„æ”¶ç›Šåˆ’åˆ†ç»“ç‚¹ï¼Œæ”¶ç›Šä¸º
   $$
   Gain = Obj_P-Obj_L-Obj_R=-\frac{1}{2} \cdot \frac{G_j^2}{H_j+\lambda}+\gamma-[-\frac{1}{2}(\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda})+2 \gamma]
   $$
   å…¶ä¸­ $Obj_P$ ä¸ºçˆ¶ç»“ç‚¹çš„å¾—åˆ†ï¼Œ$Obj_L,Obj_R$ ä¸ºå·¦å³å­©å­çš„å¾—åˆ†.

3. é€‰æ‹©æœ€å¤§å¢ç›Šè¿›è¡Œåˆ’åˆ†

   â€‹	é€‰æ‹©æœ€å¤§å¢ç›Šçš„è¿‡ç¨‹å¦‚ä¸‹ï¼š

   1. é€‰å‡ºæ‰€æœ‰å¯ä»¥ç”¨æ¥åˆ’åˆ†çš„ç‰¹å¾é›†åˆ $\mathcal F$ï¼›

   2. For feature in $\mathcal F$:

      â€‹          å°†èŠ‚ç‚¹åˆ†é…åˆ°çš„æ ·æœ¬çš„ç‰¹å¾ feature æå–å‡ºæ¥å¹¶å‡åºæ’åˆ—ï¼Œè®°ä½œ sorted_f_value_listï¼›	      For f_value in sorted_f_value_list ï¼š

      â€‹       â€‹        åœ¨ç‰¹å¾ feature ä¸ŠæŒ‰ç…§ f_value ä¸ºä¸´ç•Œç‚¹å°†æ ·æœ¬åˆ’åˆ†ä¸ºå·¦å³ä¸¤ä¸ªé›†åˆï¼›

      â€‹       â€‹        â€‹        â€‹      è®¡ç®—åˆ’åˆ†åçš„å¢ç›Šï¼›

      è¿”å›æœ€å¤§çš„å¢ç›Šæ‰€å¯¹åº”çš„ feature å’Œ f_valueã€‚ 

### åœæ­¢ç­–ç•¥

â€‹		å¯¹äºå¦‚ä½•å†³å®šä¸€ä¸ªèŠ‚ç‚¹æ˜¯å¦è¿˜éœ€è¦ç»§ç»­åˆ’åˆ†ï¼Œæˆ‘ä»¬æä¾›ä¸‹åˆ—ç­–ç•¥ï¼Œä½ å¯ä»¥é€‰æ‹©ä¸€ä¸ªæˆ–å¤šä¸ªï¼Œæˆ–è‡ªè¡Œè®¾å®šåˆç†çš„ç­–ç•¥ã€‚

- åˆ’åˆ†åå¢ç›Šå°äºæŸä¸ªé˜ˆå€¼åˆ™åœæ­¢åˆ’åˆ†ï¼›

- åˆ’åˆ†åæ ‘çš„æ·±åº¦å¤§äºæŸä¸ªé˜ˆå€¼åœæ­¢åˆ’åˆ†ï¼›

- è¯¥èŠ‚ç‚¹åˆ†é…åˆ°çš„æ ·æœ¬æ•°ç›®å°äºæŸä¸ªé˜ˆå€¼åœæ­¢åˆ†åŒ–ã€‚

  å¯¹äºæ•´ä¸ªç®—æ³•å¦‚ä½•ç»ˆæ­¢ï¼Œæˆ‘ä»¬æä¾›ä¸‹åˆ—ç­–ç•¥ï¼Œä½ å¯ä»¥é€‰æ‹©ä¸€ä¸ªæˆ–å¤šä¸ªï¼Œæˆ–è‡ªè¡Œè®¾å®šåˆç†çš„ç­–ç•¥ã€‚

- å­¦ä¹  $M$ ä¸ªé¢—å†³ç­–æ ‘ååœä¸‹æ¥ï¼›

- å½“åœ¨éªŒè¯é›†ä¸Šçš„å‡æ–¹è¯¯å·®å°äºæŸä¸ªé˜ˆå€¼æ—¶åœä¸‹æ¥ï¼›

- å½“éªŒè¯é›†å‡ºç°è¿‡æ‹Ÿåˆæ—¶åœä¸‹æ¥ã€‚

### è¯„ä»·æŒ‡æ ‡

â€‹		ä½ å¯ä»¥åœ¨å®éªŒä¸­ä»¥ä¸‹åˆ—æŒ‡æ ‡æ¥éªŒè¯ä½ çš„ç®—æ³•æ•ˆæœå’Œä¸åŒå‚æ•°å¯¹äºç»“æœçš„å½±å“

- $RMSE=\sqrt{\frac1m\sum_{i=1}^m(y_{test}^{(i)}-\hat y_{test}^{(i)})^2}\\ $ï¼Œè¶Šå°è¶Šå¥½

- $R^2=1-\frac{ \sum_{i=1}^m(y_{test}^{(i)}-\hat y_{test}^{(i)})^2}{\sum_{i=1}^m(\bar y_{test}-\hat y_{test}^{(i)})^2}=1-\frac{MSE(\hat y_{test},y_{test})}{Var(y_{test})}\\ $ï¼Œè¶Šå¤§è¶Šå¥½

- è¿è¡Œæ—¶é—´



## ä¸‰ã€å®éªŒæ­¥éª¤

1. #### è¯»å–å¹¶è·å–æ•°æ®é›†ä¿¡æ¯

   â€‹		ä½¿ç”¨pandasçš„read_csvæ¥è¯»å–æ•°æ®ï¼Œå› ä¸ºè¯¥æ•°æ®é›†æ²¡æœ‰æ ‡ç­¾ï¼Œæ•…è®¾ç½®"head=None"ã€‚	

   â€‹		æŸ¥çœ‹æ•°æ®é›†ä¿¡æ¯ï¼Œå¯çŸ¥è¯¥æ•°æ®é›†æœ‰7154 æ¡ 41 ç»´çš„æ•°æ®ï¼Œå…¶ä¸­å‰ 40 åˆ—ä¸º featureï¼Œæœ€åä¸€åˆ—ä¸º labelã€‚å‡ä¸ºfloatæˆ–intå‹ï¼Œä¸”ä¸å«ç¼ºå¤±å€¼ã€‚

   

2. #### æ„å»ºæ¨¡å‹

   â€‹		ä¸»è¦å®ç°äº†ä¸‰ä¸ªç±»ï¼š`class Node`ã€`class Decision Tree`ã€`class XGBoost`ã€‚

   ##### ï¼ˆ1ï¼‰class Node

   â€‹		`class Node`å¯¹åº”XGBoostä¸­åŸºæ¨¡å‹å†³ç­–æ ‘çš„èŠ‚ç‚¹ã€‚

   ```python
   class Node(object):
       def __init__(self):
           self.left = None  # å·¦å­©å­
           self.right = None  # å³å­©å­
           self.feature = None  # åˆ’åˆ†ç‰¹å¾
           self.value = None  # åˆ’åˆ†å€¼
           self.w = None  # æƒå€¼
           self.isleaf = False  # æ˜¯å¦ä¸ºå¶èŠ‚ç‚¹
           self.depth = None  # èŠ‚ç‚¹æ·±åº¦
   ```

   

   ##### ï¼ˆ2ï¼‰class DecisionTree

   â€‹		`class DecisionTree`ä¸ºXGBoostçš„åŸºæ¨¡å‹ï¼Œå®šä¹‰å¦‚ä¸‹ï¼š

   ```python
   class DecisionTree(object):
       def __init__(self, data, gamma, Lambda, max_depth, min_gain, min_num):
           self.gamma = gamma
           self.Lambda = Lambda
           self.max_depth = max_depth
           self.feature = None
           self.value = None
           self.min_gain = min_gain
           self.min_num = min_num
           self.root = self.BuildTree(data, 0)
   ```

   â€‹		æ¨¡å‹å‚æ•°å¦‚ä¸Šï¼Œå…¶ä¸­dataä¸º$[X,y,y_{t-1}]$ç»„æˆçš„æ•°ç»„ã€‚

   â€‹		è¯¥ç±»ä¸­åŒ…å«ä»¥ä¸‹å‡½æ•°ã€‚

   ```python
   def GetObj(self, G, H):
       return -0.5 * (G ** 2) / (H + self.Lambda) + self.gamma
   
   def GetWeight(self, data):
       G = -2 * np.sum(data[:, -2] - data[:, -1])
       H = 2 * data.shape[0]
       return -G / (H + self.Lambda)
   ```

   â€‹		`GetObj(self, G, H)`è®¡ç®—å¯¹åº”çš„è¦ä¼˜åŒ–çš„ç›®æ ‡å‡½æ•°ï¼Œ`GetWeight(self, data)`è®¡ç®—æœ€ä¼˜çš„æƒå€¼ã€‚

   ```python
   def ChooseBestSplit(self, data, depth):
       # æ ‘çš„æ·±åº¦å¤§äºæŸä¸ªé˜ˆå€¼, åœæ­¢åˆ’åˆ†
       if depth > self.max_depth:
           return None, self.GetWeight(data)
   
       y_t, y = data[:, -1], data[:, -2]
   
       # æ‰€æœ‰ç‰¹å¾å€¼éƒ½ç›¸åŒï¼Œåœæ­¢åˆ’åˆ†
       if len(set(y.T.tolist())) == 1:
           return None, self.GetWeight(data)
   
       m, n = np.shape(data)
   
       # æ ·æœ¬æ•°ç›®å°äºæŸä¸ªé˜ˆå€¼ï¼Œåœæ­¢åˆ’åˆ†
       if m < self.min_num:
           return None, self.GetWeight(data)
   
       G = np.sum(-2 * (y - y_t))
       H = 2 * m
       Obj_p = self.GetObj(G, H)
       max_gain = float("-inf")
       BestFeature = 0
       BestValue = 0
   
       # éå†å±æ€§
       for feature in range(n - 2):
           # ç‰¹å¾å€¼æ’åº
           tmp = np.c_[data[:, feature], -2 * (y - y_t)]
           sorted_value_list = tmp[np.argsort(tmp[:, 0])]
           Gl, Gr, Hl, Hr = 0, G, 0, H
           # éå†ç‰¹å¾å€¼
           for i in range(sorted_value_list.shape[0]):
               # <=i,åˆ’åˆ†åˆ°å·¦å­æ ‘
               Gl += sorted_value_list[i, -1]
               Gr = G - Gl
               Hl += 2
               Hr = H - Hl
               Obj_l = self.GetObj(Gl, Hl)
               Obj_r = self.GetObj(Gr, Hr)
               gain = Obj_p - Obj_l - Obj_r
               if gain > max_gain:
                   max_gain = gain
                   BestFeature = feature
                   BestValue = sorted_value_list[i, 0]
   
       # å¢ç›Šå°äºæŸä¸ªé˜ˆå€¼, åœæ­¢åˆ’åˆ†
       if max_gain < self.min_gain:
           return None, self.GetWeight(data)
   
       return BestFeature, BestValue
   ```

   â€‹		`ChooseBestSplit(self, data, depth)`éå†dataä¸­æ‰€æœ‰å¯ä»¥ç”¨æ¥åˆ’åˆ†çš„ç‰¹å¾é›†åˆã€‚å¯¹æ¯ä¸ªç‰¹å¾ï¼Œå…ˆå¯¹ç‰¹å¾å€¼è¿›è¡Œæ’åºï¼Œæ¥ç€éå†æ’åºåçš„ç‰¹å¾å€¼ï¼Œä»¥è¯¥ç‰¹å¾å€¼ä¸ºåˆ†ç•Œçº¿å°†dataåˆ’åˆ†ä¸ºå·¦å³å­æ ‘ï¼Œè®¡ç®—åˆ’åˆ†åçš„å¢ç›Šï¼Œè¿”å›æœ€å¤§çš„å¢ç›Šæ‰€å¯¹åº”çš„ feature å’Œ value ã€‚

   â€‹		æ’åºæ—¶ä½¿ç”¨tmpæ•°ç»„å­˜å‚¨ç‰¹å¾å€¼ä¸æ®‹å·®ï¼Œå¹¶ä½¿ç”¨np.argsort()å‡½æ•°æå–æ’åˆ—å‰å¯¹åº”çš„ç´¢å¼•ã€‚ç¬¬iæ¬¡éå†æ—¶ï¼Œå› ä¸ºå€¼å·²ç»æ˜¯æ’å¥½åºçš„ï¼Œæ•…`Gl += sorted_value_list[i, -1],Gr = G - Gl,Hl += 2,Hr = H - Hl`ï¼Œå¤§å¤§å‡å°‘äº†è®¡ç®—Objçš„è€—æ—¶ã€‚

   â€‹		åœ¨æ‰€æœ‰ç‰¹å¾å€¼éƒ½ç›¸åŒã€æ ‘çš„æ·±åº¦å¤§äºæŸä¸ªé˜ˆå€¼ã€æ ·æœ¬æ•°ç›®å°äºæŸä¸ªé˜ˆå€¼ã€å¢ç›Šå°äºæŸä¸ªé˜ˆå€¼æ—¶ï¼Œåœæ­¢åˆ’åˆ†ã€‚

   ```python
   def BuildTree(self, data, depth):
       # é€‰æ‹©æœ€ä¼˜åŒ–åˆ’åˆ†
       feature, value = self.ChooseBestSplit(data, depth)
       
       # æ»¡è¶³åœæ­¢æ¡ä»¶ï¼Œè¿”å›å¶èŠ‚ç‚¹
       if feature == None:
           leaf = Node()
           leaf.depth = depth
           leaf.isleaf = True
           leaf.w = self.GetWeight(data)
           return leaf
       
       # åˆ’åˆ†åèµ‹å€¼
       else:
           root = Node()
           root.depth = depth
           root.feature = feature
           root.value = value
           left = data[np.nonzero(data[:, feature] <= value)[0], :]
           right = data[np.nonzero(data[:, feature] > value)[0], :]
           root.left = self.BuildTree(left, depth + 1)
           root.right = self.BuildTree(right, depth + 1)
           return root
   ```

   â€‹		`BuildTree(self, data, depth)`é€’å½’åˆ›å»ºä¸€æ£µå†³ç­–æ ‘ã€‚è‹¥æ»¡è¶³åœæ­¢æ¡ä»¶ï¼Œåˆ™è¿”å›å¶èŠ‚ç‚¹ã€‚

   ```python
   def infer(self, x):
       p = self.root
       while not p.isleaf:
           if x[p.feature] <= p.value:
               p = p.left
           elif x[p.feature] > p.value:
               p = p.right
       return p.w
   ```

   â€‹		`infer(self, x)`æ ¹æ®xçš„featureè¿›è¡Œæœç´¢ï¼Œå¾—åˆ°å…¶å¯¹åº”æƒå€¼ã€‚

   

   ##### ï¼ˆ3ï¼‰class XGBoost

   â€‹		XGBoostå®šä¹‰å¦‚ä¸‹ï¼š

   ```python
   class XGBoost(object):
       def __init__(self, gamma, Lambda, max_depth, tree_num, min_gain, min_num):
           self.gamma = gamma
           self.Lambda = Lambda
           self.max_depth = max_depth
           self.tree_num = tree_num  # å†³ç­–æ ‘ä¸ªæ•°
           self.min_gain = min_gain
           self.min_num = min_num
           self.TreeList = []
   ```

   â€‹		æ¨¡å‹å‚æ•°å¦‚ä¸Šï¼Œå…¶ä¸­TreeListä¿å­˜äº†å¾—åˆ°çš„å†³ç­–æ ‘ã€‚

   ```python
   def fit(self, X, y):
       y_t = np.zeros(y.shape)
       data = np.c_[X, y, y_t]
       LossList = []
       for i in range(self.tree_num):
           print(f"Bulid {i + 1} tree")
           tree = DecisionTree(data, self.gamma, self.Lambda, self.max_depth, self.min_gain, self.min_num)
           self.TreeList.append(tree)
           data[:, -1] = self.predict(X)
           loss = np.mean((y - data[:,-1]) ** 2)
           LossList.append(loss)
       return LossList, data[:, -1]
   ```

   â€‹		`fit`å¯¹æ¨¡å‹åœ¨Xå’Œyä¸Šè¿›è¡Œè®­ç»ƒã€‚åˆå§‹åŒ–y_tä¸ºå…¨0ï¼Œæ¯ç”Ÿæˆä¸€æ£µå†³ç­–æ ‘ï¼Œå°†å…¶åŠ å…¥TreeListï¼Œå¹¶ä½¿ç”¨self.predict(X)ï¼Œæ›´æ–°y_tã€‚å­¦ä¹ numæ£µæ ‘ååœæ­¢ï¼Œè¿”å›è®­ç»ƒè¿‡ç¨‹ä¸­çš„lossåˆ—è¡¨ã€‚

   ```python
   def predict(self, X):
       if len(self.TreeList) == 0:
           print("TreeList is empty!")
       else:
           m = X.shape[0]
           y_pred = np.zeros(m)
           for i in range(m):
               for tree in self.TreeList:
                   y_pred[i] += tree.infer(X[i, :])
           return y_pred
   ```

   â€‹		`predict`éå†TreeListï¼Œå¯¹tree.inferçš„ç»“æœç›¸åŠ ï¼Œå¾—åˆ°Xçš„é¢„æµ‹å€¼ã€‚

   

3. #### æ•°æ®é›†åˆ’åˆ†

   â€‹	å°†æ•°æ®é›†åˆ’åˆ†ä¸ºX_train, X_test, y_train, y_testï¼Œå…¶ä¸­è®­ç»ƒé›†ï¼šæµ‹è¯•é›† =  7 ï¼š3ã€‚

   

4. #### è®­ç»ƒ

   â€‹	å°†æ¨¡å‹åœ¨X_trainã€y_trainä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶è®¡ç®—è®­ç»ƒè¿‡ç¨‹çš„lossã€è€—æ—¶ã€$RMSEã€R^2$ï¼Œç”»å‡ºlossæ›²çº¿ã€‚

   

5. #### æµ‹è¯•

   â€‹	å°†æ¨¡å‹åœ¨X_testã€y_testä¸Šè¿›è¡Œæµ‹è¯•ï¼Œå¹¶è®¡ç®—æµ‹è¯•é›†çš„$RMSEã€R^2$ã€‚

   

## å››ã€å®éªŒç»“æœä¸åˆ†æ

ï¼ˆ1ï¼‰æœ€ä½³æ¨¡å‹

```python
gamma = 1e-6, Lambda = 7, max_depth = 2, tree_num = 12, min_gain = 1e-7, min_num = 5
```

- è®­ç»ƒè€—æ—¶ï¼š13.44s
- è®­ç»ƒé›†
  - $RMSE$ : 0.0001804
  - $R^2$ : 0.8120
  - <img src="D:\æ¡Œé¢æ–‡ä»¶\image-20221120173918302.png" alt="image-20221120173918302" style="zoom:67%;" />
- æµ‹è¯•é›†
  - $RMSE$ : 0.0001845
  - $R^2$ : 0.7896



ï¼ˆ2ï¼‰ä¸åŒå‚æ•°çš„æ¯”è¾ƒ	

| gamma | training $RMSE$ | training $RMSE$ | training $R^2$ | testing $R^2$ |
| ----- | --------------- | --------------- | -------------- | ------------- |
| 1e-5  | 0.0002252       | 0.0002204       | 0.7070         | 0.6998        |
| 1e-6  | 0.0001807       | 0.0001852       | 0.8113         | 0.7881        |
| 1e-7  | 0.0001838       | 0.0001915       | 0.8049         | 0.7733        |

â€‹		ç”±è¡¨å¯è§åœ¨gamma=1e-6å·¦å³æ•ˆæœæœ€å¥½ã€‚

| Lambda | training $RMSE$ | testing $RMSE$ | training $R^2$ | testing $R^2$ |
| ------ | --------------- | -------------- | -------------- | ------------- |
| 3      | 0.0001914       | 0.0001925      | 0.7884         | 0.7711        |
| 5      | 0.0001808       | 0.0001855      | 0.8111         | 0.7875        |
| 6      | 0.0001808       | 0.0001853      | 0.8112         | 0.7878        |
| 7      | 0.0001807       | 0.0001852      | 0.8113         | 0.7881        |
| 8      | 0.0001825       | 0.0001929      | 0.8077         | 0.7702        |

<img src="D:\æ¡Œé¢æ–‡ä»¶\image-20221120165524459.png" alt="image-20221120165524459" style="zoom: 30%;" /><img src="D:\æ¡Œé¢æ–‡ä»¶\image-20221120165547484.png" alt="image-20221120165547484" style="zoom:30%;" />

â€‹		ç”±å›¾è¡¨å¯è§ï¼Œéšç€Lambdaçš„å¢åŠ ï¼ŒRMSEå…ˆä¸‹é™åä¸Šå‡ï¼Œ$R^2$å…ˆä¸Šå‡åä¸‹é™ã€‚ä¸”æµ‹è¯•é›†ä¸Šå˜åŒ–æ˜æ˜¾ï¼Œäº§ç”Ÿäº†è¿‡æ‹Ÿåˆã€‚å–Lambda = 7ã€‚

| max_depth | training $RMSE$ | testing $RMSE$ | training $R^2$ | testing $R^2$ |
| :-------- | --------------- | -------------- | -------------- | ------------- |
| 1         | 0.0001928       | 0.0001906      | 0.7853         | 0.7755        |
| 2         | 0.0001807       | 0.0001852      | 0.8113         | 0.7881        |
| 3         | 0.0001891       | 0.0002051      | 0.7934         | 0.7401        |

â€‹		ç”±è¡¨å¯è§ï¼Œéšç€max_depthçš„å¢åŠ ï¼ŒRMSEä¸‹é™åä¸Šå‡ï¼Œ$R^2$å…ˆä¸Šå‡åä¸‹é™ã€‚å–max_depth=2ã€‚

| tree_num | training $RMSE$ | testing $RMSE$ | training $R^2$ | testing $R^2$ |
| -------- | --------------- | -------------- | -------------- | ------------- |
| 5        | 0.000193132     | 0.000193988    | 0.784565       | 0.767480      |
| 10       | 0.000180740     | 0.000185174    | 0.811327       | 0.788130      |
| 12       | 0.000180739     | 0.000185173    | 0.811327       | 0.788131      |
| 15       | 0.000180739     | 0.000185173    | 0.811327       | 0.788131      |

<img src="D:\æ¡Œé¢æ–‡ä»¶\image-20221120160735171.png" alt="image-20221120160735171" style="zoom:31%;" /><img src="D:\æ¡Œé¢æ–‡ä»¶\image-20221120161048702.png" alt="image-20221120161048702" style="zoom:30%;" />

â€‹		ç”±å›¾è¡¨å¯è§åœ¨tree_num>10å·¦å³ï¼Œæ¨¡å‹æ”¶æ•›ã€‚å–tree_num = 12ã€‚

| min_gain | training $RMSE$ | testing $RMSE$ | training $R^2$ | testing $R^2$ |
| -------- | --------------- | -------------- | -------------- | ------------- |
| 1e-6     | 0.0001925       | 0.0001955      | 0.7860         | 0.7640        |
| 1e-7     | 0.0001804       | 0.0001845      | 0.8120         | 0.7896        |
| 1e-8     | 0.0001807       | 0.0001852      | 0.8113         | 0.7881        |
| 1e-9     | 0.0001807       | 0.0001852      | 0.8113         | 0.7881        |

â€‹		ç”±è¡¨å¯è§åœ¨max_gain=1e-7å·¦å³æ•ˆæœæœ€å¥½ã€‚

| min_num | training $RMSE$ | training $R^2$ | testing $RMSE$ | testing $R^2$ |
| ------- | --------------- | -------------- | -------------- | ------------- |
| 2       | 0.0001807       | 0.8113         | 0.0001851      | 0.7881        |
| 5       | 0.0001807       | 0.8113         | 0.0001851      | 0.7881        |
| 10      | 0.0001807       | 0.8113         | 0.0001851      | 0.7881        |
| 100     | 0.0001817       | 0.8092         | 0.0001906      | 0.7756        |

â€‹		ç”±è¡¨å¯è§min_numå¯¹ç»“æœçš„å½±å“ä¸å¤§ï¼Œå–min_num=5ã€‚



ï¼ˆ3ï¼‰ä¸å†³ç­–æ ‘æ¯”è¾ƒ

â€‹		è¯¥æ¨¡å‹ä¸sklearné‡Œçš„å†³ç­–æ ‘æ¯”è¾ƒå¦‚ä¸‹ï¼š

|        | Decision Tree | XGBoost   |
| ------ | ------------- | --------- |
| $RMSE$ | 0.0002027     | 0.0001845 |
| $R^2$  | 0.7460        | 0.7896    |

â€‹		å¯è§é›†æˆæå‡äº†æ€§èƒ½ã€‚



## äº”ã€ç»“æœåˆ†æ

â€‹		XGBoostæ˜¯ä¸€ç§é«˜æ•ˆçš„æ¢¯åº¦æå‡å†³ç­–æ ‘ç®—æ³•ã€‚ä½œä¸ºä¸€ç§å‰å‘åŠ æ³•æ¨¡å‹ï¼Œæ ¸å¿ƒæ˜¯é‡‡ç”¨boostingæ€æƒ³ï¼Œå°†å¤šä¸ªå¼±å­¦ä¹ å™¨é€šè¿‡ä¸€å®šçš„æ–¹æ³•æ•´åˆä¸ºä¸€ä¸ªå¼ºå­¦ä¹ å™¨ã€‚

â€‹		è¯¥æ•°æ®é›†ä¸å¤§ï¼Œæ•…ä½¿ç”¨XGBoostå®¹æ˜“äº§ç”Ÿè¿‡æ‹Ÿåˆã€‚

â€‹		å¯ä»¥é€šè¿‡å¢å¤§$\lambda$ï¼Œå‡å°æ ‘çš„æ·±åº¦å’Œæ•°ç›®ç­‰æ¥ç¼“è§£è¿‡æ‹Ÿåˆã€‚

